#!/bin/bash

declare -a links

scrape() {
    baseUrl=$(echo $url | awk -F "/" '{print $1 "/" $2 "/" $3}')
    dotUrl=$(echo ${1} | awk -F "?" '{print $1}' | sed -E 's>(/$)>>')
    ddotUrl=$(echo ${dotUrl} | sed -E -e 's>(.*\/)[^\/]+$>\1>' -e 's>(/$)>>')
    curl -H 'Accept-Language: en-US,en;q=0.5' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8' -A "$user_agent" -s -L "$1" | grep -Eo "href=[\"'][^#][^\"']+" | sed -E "s>href=(\"|')(\".*)?>>g" | sed -E -e "s>^(\/\/).+>https:&>" -e "s>^(\.\.)[\/](.*)>${ddotUrl}\/\2>" -e "s>^[\~\/|\/](.*)>$baseUrl\/\1>" -e "/^(http|tel)/!s>^(\.\/)?(.*)>${dotUrl}\/\2>" -e "s> >%20>g"
}
vuln() {

    links=( $(for i in ${list[@]}
    do
        output=$(echo -e "\e[32m$i\e[0;97m" || echo "$i")
        echo -e "$output\n$(scrape ${i})"&
    done) )
    echo "${#links[@]}\n"
}
main() {
    if [ -p /dev/stdin ]
    then
        read url

    elif [ -z "$*" ]
    then
        banner
        echo -en "\e[1;36mEnter the URL: \e[0m"
        read url
    else
        url=$1

    fi
    [[ ! $url =~ http[s]?://.* ]] && url="https://${url}" &&echo -e "\e[1;93mNo scheme specified, using default https scheme\e[0m"
    user_agent="Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0"
    list=( $(scrape $url) )
    count=${#list[@]}
    if [[ $count == 0 ]]
    then
        echo -e "\e[1;91mNO LINKS FOUND IN THIS DOMAIN :(\e[0m\nURL used: $url" >&2
        exit 5
    fi
    vuln
}
main
